# -*- coding: utf-8 -*-
"""CNN_Clasificador_PerrosGatos_EarlyStopping_SpaAtt_MaxPooling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1szQjCDcPd731AqgGUmuIs_oKIDwhtQfh

# Clasificador de imágenes de perros y gatos con CNN con Atención Espacial.
"""

import numpy as np

"""## Explorando los datos
Se descargan los datos en un fichero zip con 2000 imágenes de perros y gatos. Se descarga el zip y se extraen el directorio /tmp de la máquina virtual local. Estos datos son un subconjunto de
["Dogs vs. Cats" dataset](https://www.kaggle.com/c/dogs-vs-cats/data) disponible en Kaggle.
"""

!wget --no-check-certificate \
    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \
    -O /tmp/cats_and_dogs_filtered.zip

import os
import zipfile

local_zip = '/tmp/cats_and_dogs_filtered.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

"""Los contenidos del zip son extraídos en el directorio `/tmp/cats_and_dogs_filtered`, conteniendo los directorios `train` and `validation`, los cuales a su vez contienen los subdirectorios `train` and `validation`."""

base_dir = '/tmp/cats_and_dogs_filtered'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# Directorio con las imágenes de entrenamiento de la categoría gato
train_cats_dir = os.path.join(train_dir, 'cats')

# Directorio con las imágenes de entrenamiento de la categoría perro
train_dogs_dir = os.path.join(train_dir, 'dogs')

# Directorio con las imágenes de validación de la categoría gato
validation_cats_dir = os.path.join(validation_dir, 'cats')

# Directorio con las imágenes de validación de la categoría perro
validation_dogs_dir = os.path.join(validation_dir, 'dogs')

"""Visualizando los nombres de las imágenes en los diferentes directorios."""

train_cat_fnames = os.listdir(train_cats_dir)
print(train_cat_fnames[:10])

train_dog_fnames = os.listdir(train_dogs_dir)
train_dog_fnames.sort()
print(train_dog_fnames[:10])

"""Número total de imágenes en cada categoría y directorio."""

print('total training cat images:', len(os.listdir(train_cats_dir)))
print('total training dog images:', len(os.listdir(train_dogs_dir)))
print('total validation cat images:', len(os.listdir(validation_cats_dir)))
print('total validation dog images:', len(os.listdir(validation_dogs_dir)))

"""Para gatos y perro hay 1000 imágenes en el entrenamiento y 500 en la validación."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Parámetros de la imagen, configuración 4x4
nrows = 4
ncols = 4

# Indice sobre las imágenes.
pic_index = 0

"""Se muestran 8 imágenes de cada tipo.

## Definición de la CNN

Las imágenes color se escalan a tamaño image_size X image_size píxeles, aunque en realidad son objetos 150x150x3 debido a los 3 colores RGB.
"""

#Se configura el tamaño de las imágenes.
image_size=150

from tensorflow.keras import layers
from tensorflow.keras import Model

from tensorflow.keras.layers import Layer
import tensorflow as tf

class SpatialAttention(Layer):
    def __init__(self, **kwargs):
        super(SpatialAttention, self).__init__(**kwargs)

    def build(self, input_shape):
        # Define un kernel (peso) de forma (1, 1, input_shape[-1], 1)
        # input_shape[-1] será 3 para imágenes RGB
        self.kernel = self.add_weight(name='kernel',
                                      shape=(1,1,input_shape[-1],1),
                                      initializer='uniform',
                                      trainable=True)
        super(SpatialAttention, self).build(input_shape)

    def call(self, x):
        # Aplica una convolución 2D usando el kernel definido
        attention = tf.nn.sigmoid(tf.nn.conv2d(x, self.kernel, strides=[1,1], padding='SAME'))
        # Multiplica la entrada original por la máscara de atención
        return x * attention

# Capa de entrada de datos: image_size x image_size x 3
img_input = layers.Input(shape=(image_size, image_size, 3))

# Primera capa de atención
SpaAtt = SpatialAttention()(img_input)
print('SpaAtt',SpaAtt.shape)

# Bloque convolucional formado por:
# Capa convolucional con 16 filtros y tamaño 3x3.
# Capa max-pooling de tamaño 2x2.
x = layers.Conv2D(16, 3, activation='relu')(SpaAtt)
x = layers.MaxPooling2D(2)(x)

# Bloque convolucional formado por:
# Capa convolucional con 32 filtros y tamaño 3x3.
# Capa max-pooling de tamaño 2x2.
x = layers.Conv2D(32, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

# Bloque convolucional formado por:
# Capa convolucional con 64 filtros y tamaño 3x3.
# Capa max-pooling de tamaño 2x2.
x = layers.Conv2D(64, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

# Bloque convolucional formado por:
# Capa convolucional con 64 filtros y tamaño 3x3.
# Capa max-pooling de tamaño 2x2.
x = layers.Conv2D(128, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

# En todos las capas anteriores se utiliza la función
# de activación relu.

# A continuación se añaden dos capas densas,
# la primera de las cuales tiene una función de activación relu.
# Finalmente la capa de salida tiene una sola neurona para producir
# un único escalar con función de activación sigmoide para producir
# una salida en el rango entre 0 y 1.

# Se aplana el objeto de forma que en vez se tener un tensor 3D sea 1D.
x = layers.Flatten()(x)

# Capa densa con función de activación relu.
x = layers.Dense(128, activation='relu')(x)

# Capa densa con función de activación relu.
x = layers.Dense(32, activation='relu')(x)

# Capa de salida de una única neurona y función de activación sigmoide.
output = layers.Dense(1, activation='sigmoid')(x)

# Crear el modelo:
model = Model(img_input, output)

model.summary()

# Entrenar el modelo.
from tensorflow.keras.optimizers import RMSprop

model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(learning_rate=0.001),
              metrics=['acc'])

batchsize=64
nepochs = 50

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Todas las imágenes se reescalan en 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# El flujo de imágenes de entrenamiento usando grupos de 20
train_generator = train_datagen.flow_from_directory(
        train_dir,  # Directorio donde encontrar el conjunto de entremianto
        target_size=(image_size, image_size),  # Reescalar imágenes a 150x150
        batch_size=batchsize,
        # Puesto que el error es la entropia cruzada binaria, se necesitan etiquetas binarias
        class_mode='binary')

# El flujo de imágenes de validación usando grupos de 20
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(image_size, image_size),
        batch_size=batchsize,
        shuffle=False, # El conjunto de validación no se baraja
        class_mode='binary')

"""### Entrenamiento"""

callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', verbose=1),
        tf.keras.callbacks.ModelCheckpoint(filepath='model_CNN_catdogs.keras', verbose=1, save_best_only=True)
        ]

history=model.fit(train_generator, validation_data=validation_generator,
                  epochs=nepochs,
                  callbacks =[callbacks],
                  verbose=1)

"""## Visualización de las representaciones intermedias

Se visualiza las características intermedias aprendidas por la CNN. Se puede observar como la entrada se transforma al pasar através de las capas. Se selecciona una imagen aleatoria del conjunto de entrenamiento y se visualiza durante su transformación. Cada fila corresponde con una capa y cada imagen en la fila con un filtro.

## Evaluar la calidad del modelo

### Error y exactitud.
Se representan el error y la exactitud para los conjuntos de entrenamiento y validación.
"""

# Se recuperan el error y la exactitud de los conjuntos de
# entrenamiento y validación
acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

# Número de épocas
epochs = range(len(acc))

# Representación de la exactitud por época de los conjutos de
# entrenamiento y validación plt.plot(epochs, acc)
plt.plot(epochs, acc)
plt.plot(epochs, val_acc)
plt.title('Training and validation accuracy')

plt.figure()

# Representación del error por época de los conjutos de
# entrenamiento y validación
plt.plot(epochs, loss)
plt.plot(epochs, val_loss)
plt.title('Training and validation loss')

Y_pred = model.predict(validation_generator)

# Visualización de los pronósticos.
plt.figure(1,figsize=(6,6))
#plt.style.use('seaborn-deep')
plt.hist(Y_pred[:500],50,histtype='step',color='darkorange', label= "Gatos")
plt.hist(Y_pred[500:],50,histtype='step',color='blue', label= "Perros")
plt.ylabel('Número de casos')
plt.xlabel('Predicción')
plt.title("Clasificador CNN Perros-Gatos")
plt.xlim(-0.01,1.01)
plt.legend( loc='upper center')

#from google.colab import files
#plt.savefig('ClasificadorCNNPerrosGatos.eps')
#files.download('ClasificadorCNNPerrosGatos.eps') # Descomentar para descargar localmente

"""### Matrix de confusión
La matrix de confusión es un sistema para evaluar la calidad de la predicción en resultados categóricos.
"""

from sklearn.metrics import classification_report, confusion_matrix

"""Se generan las etiquetas de las imágenes del conjunto de validación. Primero los valores numéricos del pronósitico, el cual es separado en dos vectores iguales. La primera mitad corresponde a los gatos y la segunda mitad a los perros. Finalmente se transforma los valores numéricos en el rango de 0 a 1, en etiquetas 0 ó 1, en función si es menor o mayor de 0.5. Este umbral es completamente arbitrario."""

Y_pred_cats=Y_pred[:500]
Y_pred_dogs=Y_pred[500:]

Y_pred[Y_pred<=0.5] = 0
Y_pred[Y_pred>0.5] = 1

"""Se construye el vector que corresponde con las etiquetas originales del conjunto de validación"""

Y_label_ori= np.zeros(len(Y_pred))
Y_label_ori[500:]=1

cm=confusion_matrix(Y_label_ori, Y_pred)
print(cm)

# Trues
TP = cm[1,1]; print(TP)
TN = cm[0,0]; print(TN)
# Falses
FP = cm[0,1]; print(FP)
FN = cm[1,0]; print(FN)

# Sensitivity, hit rate, recall, or true positive rate
TPR = TP/(TP+FN)
print('Sensitivity or true positive rate',TPR)
# Specificity or true negative rate
TNR = TN/(TN+FP)
print('Specificity or true negative rate',TNR)
# Precision or positive predictive value
PPV = TP/(TP+FP)
print('Precision or positive predictive value',np.round(PPV,3))
# Negative predictive value
NPV = TN/(TN+FN)
print('Negative predictive value',np.round(NPV,3))
# Fall out or false positive rate
FPR = FP/(FP+TN)
print('False positive rate',FPR)
# False negative rate
FNR = FN/(TP+FN)
print('False negative rate',FPR)
# False discovery rate
FDR = FP/(TP+FP)
print('False discovery rate',FDR)

# Overall accuracy
ACC = (TP+TN)/(TP+FP+FN+TN)
print('Accuracy',ACC)

"""## Information of Spatial Attention layer"""

# Obtener los pesos de la capa de atención
attention_weights = model.get_layer('spatial_attention').get_weights()[0]

print(attention_weights.shape)
print(attention_weights)

# Crear un modelo parcial para obtener la salida de la capa de atención
attention_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('spatial_attention').output)

image_list = []
image_labels = []

for images,labels in next(zip(train_generator)):
  image_labels_pred = model.predict(images)
  for i in range(batchsize): # can't be greater than batch size
    image_list.append(images[i])
    image_labels.append(labels[i])
    print(labels[i],image_labels_pred[i])

image_list = np.array(image_list)

print(type(image_list),image_list.shape)

# Obtener la máscara de atención para la imagen de prueba
attention_map = attention_model.predict(image_list)

print(attention_map.shape)

attention_map_mean = np.mean(attention_map, axis=-1)
print(attention_map_mean.shape)
print(np.min(attention_map_mean),np.max(attention_map_mean))

attention_map_mean = np.where(attention_map_mean<0.30,0,attention_map_mean)

idx = 4
labeltext=['cat','dog']

print('Original label', image_labels[idx], labeltext[int(image_labels[idx])],
      ', Predicted as: ', str(image_labels_pred[idx]),
      labeltext[np.squeeze(np.where(image_labels_pred[idx] > 0.5, 1, 0))]
      )

# Mostrar la imagen original y la máscara de atención
fig, ax = plt.subplots(1, 2, figsize=(8,4))

fig.text(0.5, 0.96, "Class " + str(labeltext[int(image_labels[idx])]) + ' predicted as ' + labeltext[np.squeeze(np.where(image_labels_pred[idx] > 0.5, 1, 0))],
         ha='center', va='top', fontsize=14)

# Original
ax[0].imshow(image_list[idx])
ax[0].set_title("Imagen Original")
#ax[0].set_title("Imagen Original: "+str(labeltext[int(image_labels[idx])]), ', predicted as:',labeltext[np.squeeze(np.where(image_labels_pred[idx] > 0.5, 1, 0))])

# attention map
ax[1].imshow(attention_map_mean[idx], cmap='jet', alpha=0.5)
ax[1].set_title("Máscara de Atención:")

# Adjust the layout to reduce white space between subplots
plt.subplots_adjust(left=0.1, right=0.9, top=0.85, bottom=0.1, wspace=0.2)


#plt.savefig('CNN_classification_CatsDogs_SpaAtt_'+str(idx)+'.jpg')